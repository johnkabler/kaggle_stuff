{
 "metadata": {
  "name": "Titanic"
 }, 
 "nbformat": 2, 
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "import csv as csv"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 19
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "import numpy as np"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 20
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "csv_file_object = csv.reader(open('train.csv', 'rb'))"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 21
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "header = csv_file_object.next()"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 22
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "data = []"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 23
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "for row in csv_file_object:", 
      "    data.append(row)"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 24
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "After iterating through the csv_file_object(its a generator) and adding each row to the data list", 
      "we will now convert to an array.  Each item is currently a string."
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "data = np.array(data)"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 25
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "data[0::,1]"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 26, 
       "text": [
        "array(['0', '1', '1', '1', '0', '0', '0', '0', '1', '1', '1', '1', '0',", 
        "       '0', '0', '1', '0', '1', '0', '1', '0', '1', '1', '1', '0', '1',", 
        "       '0', '0', '1', '0', '0', '1', '1', '0', '0', '0', '1', '0', '0',", 
        "       '1', '0', '0', '0', '1', '1', '0', '0', '1', '0', '0', '0', '0',", 
        "       '1', '1', '0', '1', '1', '0', '1', '0', '0', '1', '0', '0', '0',", 
        "       '1', '1', '0', '1', '0', '0', '0', '0', '0', '1', '0', '0', '0',", 
        "       '1', '1', '0', '1', '1', '0', '1', '1', '0', '0', '1', '0', '0',", 
        "       '0', '0', '0', '0', '0', '0', '1', '1', '0', '0', '0', '0', '0',", 
        "       '0', '0', '1', '1', '0', '1', '0', '0', '0', '0', '0', '0', '0',", 
        "       '0', '0', '0', '0', '0', '0', '1', '0', '1', '0', '1', '1', '0',", 
        "       '0', '0', '0', '1', '0', '0', '1', '0', '0', '0', '0', '1', '1',", 
        "       '0', '0', '0', '1', '0', '0', '0', '0', '1', '0', '0', '0', '0',", 
        "       '1', '0', '0', '0', '0', '1', '0', '0', '0', '1', '1', '0', '0',", 
        "       '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0',", 
        "       '0', '1', '1', '0', '1', '1', '0', '0', '1', '0', '1', '1', '1',", 
        "       '1', '0', '0', '1', '0', '0', '0', '0', '0', '1', '0', '0', '1',", 
        "       '1', '1', '0', '1', '0', '0', '0', '1', '1', '0', '1', '0', '1',", 
        "       '0', '0', '0', '1', '0', '1', '0', '0', '0', '1', '0', '0', '1',", 
        "       '0', '0', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '0',", 
        "       '1', '1', '0', '0', '0', '0', '0', '0', '1', '1', '1', '1', '1',", 
        "       '0', '1', '0', '0', '0', '0', '0', '1', '1', '1', '0', '1', '1',", 
        "       '0', '1', '1', '0', '0', '0', '1', '0', '0', '0', '1', '0', '0',", 
        "       '1', '0', '1', '1', '1', '1', '0', '0', '0', '0', '0', '0', '1',", 
        "       '1', '1', '1', '0', '1', '0', '1', '1', '1', '0', '1', '1', '1',", 
        "       '0', '0', '0', '1', '1', '0', '1', '1', '0', '0', '1', '1', '0',", 
        "       '1', '0', '1', '1', '1', '1', '0', '0', '0', '1', '0', '0', '1',", 
        "       '1', '0', '1', '1', '0', '0', '0', '1', '1', '1', '1', '0', '0',", 
        "       '0', '0', '0', '0', '0', '1', '0', '1', '1', '0', '0', '0', '0',", 
        "       '0', '0', '1', '1', '1', '1', '1', '0', '0', '0', '0', '1', '1',", 
        "       '0', '0', '0', '1', '1', '0', '1', '0', '0', '0', '1', '0', '1',", 
        "       '1', '1', '0', '1', '1', '0', '0', '0', '0', '1', '1', '0', '0',", 
        "       '0', '0', '0', '0', '1', '0', '0', '0', '0', '1', '0', '1', '0',", 
        "       '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '0',", 
        "       '1', '1', '1', '1', '0', '0', '1', '0', '1', '0', '0', '1', '0',", 
        "       '0', '1', '1', '1', '1', '1', '1', '1', '0', '0', '0', '1', '0',", 
        "       '1', '0', '1', '1', '0', '1', '0', '0', '0', '0', '0', '0', '0',", 
        "       '0', '1', '0', '0', '1', '1', '0', '0', '0', '0', '0', '1', '0',", 
        "       '0', '0', '1', '1', '0', '1', '0', '0', '1', '0', '0', '0', '0',", 
        "       '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1', '0', '1',", 
        "       '1', '0', '1', '1', '0', '1', '1', '0', '0', '1', '0', '1', '0',", 
        "       '1', '0', '0', '1', '0', '0', '1', '0', '0', '0', '1', '0', '0',", 
        "       '1', '0', '1', '0', '1', '0', '1', '1', '0', '0', '1', '0', '0',", 
        "       '1', '1', '0', '1', '1', '0', '0', '1', '1', '0', '1', '0', '1',", 
        "       '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '1',", 
        "       '1', '1', '0', '0', '1', '1', '0', '1', '1', '1', '0', '0', '0',", 
        "       '1', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '1', '0',", 
        "       '0', '1', '1', '0', '0', '0', '1', '0', '0', '1', '1', '1', '0',", 
        "       '0', '1', '0', '0', '1', '0', '0', '1', '0', '0', '1', '1', '0',", 
        "       '0', '0', '0', '1', '0', '0', '1', '0', '1', '0', '0', '1', '0',", 
        "       '0', '0', '0', '0', '1', '0', '1', '1', '1', '0', '1', '0', '1',", 
        "       '0', '1', '0', '1', '0', '0', '0', '0', '0', '0', '1', '0', '0',", 
        "       '0', '1', '0', '0', '0', '0', '1', '1', '0', '0', '1', '0', '0',", 
        "       '0', '1', '0', '1', '0', '1', '0', '0', '0', '0', '0', '0', '0',", 
        "       '1', '1', '1', '1', '0', '0', '0', '0', '1', '0', '0', '1', '1',", 
        "       '0', '0', '0', '0', '1', '1', '1', '1', '1', '0', '1', '0', '0',", 
        "       '0', '1', '1', '0', '0', '1', '0', '0', '0', '1', '0', '1', '1',", 
        "       '0', '0', '1', '0', '0', '0', '0', '0', '0', '1', '0', '0', '1',", 
        "       '0', '1', '0', '1', '0', '0', '1', '0', '0', '1', '1', '0', '0',", 
        "       '1', '1', '0', '0', '0', '1', '0', '0', '1', '1', '0', '1', '0',", 
        "       '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '1', '0', '1',", 
        "       '1', '1', '0', '0', '0', '0', '1', '0', '1', '0', '0', '0', '0',", 
        "       '0', '0', '0', '1', '1', '0', '0', '0', '1', '1', '1', '1', '0',", 
        "       '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0',", 
        "       '0', '1', '1', '0', '1', '0', '0', '0', '1', '1', '1', '1', '1',", 
        "       '0', '0', '0', '1', '0', '0', '1', '1', '0', '0', '1', '0', '0',", 
        "       '0', '0', '0', '0', '1', '0', '0', '0', '1', '0', '1', '1', '1',", 
        "       '1', '0', '0', '0', '1', '0', '0', '1', '1', '0', '0', '1', '0',", 
        "       '1', '0', '0', '1', '1', '0', '0', '0', '1', '1', '0', '0', '0',", 
        "       '0', '0', '0', '1', '0', '1', '0'], ", 
        "      dtype='|S82')"
       ]
      }
     ], 
     "prompt_number": 26
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "number_passengers = np.size(data[0::,0].astype(np.float))", 
      "number_passengers"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 27, 
       "text": [
        "891"
       ]
      }
     ], 
     "prompt_number": 27
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "number_survived = np.sum(data[0::,1].astype(np.float))", 
      "number_survived"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 28, 
       "text": [
        "342.0"
       ]
      }
     ], 
     "prompt_number": 28
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "proportion_survivors = number_survived / number_passengers", 
      "proportion_survivors"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 29, 
       "text": [
        "0.38383838383838381"
       ]
      }
     ], 
     "prompt_number": 29
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "header[3]"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 30, 
       "text": [
        "'Name'"
       ]
      }
     ], 
     "prompt_number": 30
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "header[0]"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 31, 
       "text": [
        "'PassengerId'"
       ]
      }
     ], 
     "prompt_number": 31
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "for column in header:", 
      "    print column"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "PassengerId", 
        "Survived", 
        "Pclass", 
        "Name", 
        "Sex", 
        "Age", 
        "SibSp", 
        "Parch", 
        "Ticket", 
        "Fare", 
        "Cabin", 
        "Embarked"
       ]
      }
     ], 
     "prompt_number": 32
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "data[0::, 5]"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 51, 
       "text": [
        "array(['22', '38', '26', '35', '35', '', '54', '2', '27', '14', '4', '58',", 
        "       '20', '39', '14', '55', '2', '', '31', '', '35', '34', '15', '28',", 
        "       '8', '38', '', '19', '', '', '40', '', '', '66', '28', '42', '',", 
        "       '21', '18', '14', '40', '27', '', '3', '19', '', '', '', '', '18',", 
        "       '7', '21', '49', '29', '65', '', '21', '28.5', '5', '11', '22',", 
        "       '38', '45', '4', '', '', '29', '19', '17', '26', '32', '16', '21',", 
        "       '26', '32', '25', '', '', '0.83', '30', '22', '29', '', '28', '17',", 
        "       '33', '16', '', '23', '24', '29', '20', '46', '26', '59', '', '71',", 
        "       '23', '34', '34', '28', '', '21', '33', '37', '28', '21', '', '38',", 
        "       '', '47', '14.5', '22', '20', '17', '21', '70.5', '29', '24', '2',", 
        "       '21', '', '32.5', '32.5', '54', '12', '', '24', '', '45', '33',", 
        "       '20', '47', '29', '25', '23', '19', '37', '16', '24', '', '22',", 
        "       '24', '19', '18', '19', '27', '9', '36.5', '42', '51', '22', '55.5',", 
        "       '40.5', '', '51', '16', '30', '', '', '44', '40', '26', '17', '1',", 
        "       '9', '', '45', '', '28', '61', '4', '1', '21', '56', '18', '', '50',", 
        "       '30', '36', '', '', '9', '1', '4', '', '', '45', '40', '36', '32',", 
        "       '19', '19', '3', '44', '58', '', '42', '', '24', '28', '', '34',", 
        "       '45.5', '18', '2', '32', '26', '16', '40', '24', '35', '22', '30',", 
        "       '', '31', '27', '42', '32', '30', '16', '27', '51', '', '38', '22',", 
        "       '19', '20.5', '18', '', '35', '29', '59', '5', '24', '', '44', '8',", 
        "       '19', '33', '', '', '29', '22', '30', '44', '25', '24', '37', '54',", 
        "       '', '29', '62', '30', '41', '29', '', '30', '35', '50', '', '3',", 
        "       '52', '40', '', '36', '16', '25', '58', '35', '', '25', '41', '37',", 
        "       '', '63', '45', '', '7', '35', '65', '28', '16', '19', '', '33',", 
        "       '30', '22', '42', '22', '26', '19', '36', '24', '24', '', '23.5',", 
        "       '2', '', '50', '', '', '19', '', '', '0.92', '', '17', '30', '30',", 
        "       '24', '18', '26', '28', '43', '26', '24', '54', '31', '40', '22',", 
        "       '27', '30', '22', '', '36', '61', '36', '31', '16', '', '45.5',", 
        "       '38', '16', '', '', '29', '41', '45', '45', '2', '24', '28', '25',", 
        "       '36', '24', '40', '', '3', '42', '23', '', '15', '25', '', '28',", 
        "       '22', '38', '', '', '40', '29', '45', '35', '', '30', '60', '', '',", 
        "       '24', '25', '18', '19', '22', '3', '', '22', '27', '20', '19', '42',", 
        "       '1', '32', '35', '', '18', '1', '36', '', '17', '36', '21', '28',", 
        "       '23', '24', '22', '31', '46', '23', '28', '39', '26', '21', '28',", 
        "       '20', '34', '51', '3', '21', '', '', '', '33', '', '44', '', '34',", 
        "       '18', '30', '10', '', '21', '29', '28', '18', '', '28', '19', '',", 
        "       '32', '28', '', '42', '17', '50', '14', '21', '24', '64', '31',", 
        "       '45', '20', '25', '28', '', '4', '13', '34', '5', '52', '36', '',", 
        "       '30', '49', '', '29', '65', '', '50', '', '48', '34', '47', '48',", 
        "       '', '38', '', '56', '', '0.75', '', '38', '33', '23', '22', '',", 
        "       '34', '29', '22', '2', '9', '', '50', '63', '25', '', '35', '58',", 
        "       '30', '9', '', '21', '55', '71', '21', '', '54', '', '25', '24',", 
        "       '17', '21', '', '37', '16', '18', '33', '', '28', '26', '29', '',", 
        "       '36', '54', '24', '47', '34', '', '36', '32', '30', '22', '', '44',", 
        "       '', '40.5', '50', '', '39', '23', '2', '', '17', '', '30', '7',", 
        "       '45', '30', '', '22', '36', '9', '11', '32', '50', '64', '19', '',", 
        "       '33', '8', '17', '27', '', '22', '22', '62', '48', '', '39', '36',", 
        "       '', '40', '28', '', '', '24', '19', '29', '', '32', '62', '53',", 
        "       '36', '', '16', '19', '34', '39', '', '32', '25', '39', '54', '36',", 
        "       '', '18', '47', '60', '22', '', '35', '52', '47', '', '37', '36',", 
        "       '', '49', '', '49', '24', '', '', '44', '35', '36', '30', '27',", 
        "       '22', '40', '39', '', '', '', '35', '24', '34', '26', '4', '26',", 
        "       '27', '42', '20', '21', '21', '61', '57', '21', '26', '', '80',", 
        "       '51', '32', '', '9', '28', '32', '31', '41', '', '20', '24', '2',", 
        "       '', '0.75', '48', '19', '56', '', '23', '', '18', '21', '', '18',", 
        "       '24', '', '32', '23', '58', '50', '40', '47', '36', '20', '32',", 
        "       '25', '', '43', '', '40', '31', '70', '31', '', '18', '24.5', '18',", 
        "       '43', '36', '', '27', '20', '14', '60', '25', '14', '19', '18',", 
        "       '15', '31', '4', '', '25', '60', '52', '44', '', '49', '42', '18',", 
        "       '35', '18', '25', '26', '39', '45', '42', '22', '', '24', '', '48',", 
        "       '29', '52', '19', '38', '27', '', '33', '6', '17', '34', '50', '27',", 
        "       '20', '30', '', '25', '25', '29', '11', '', '23', '23', '28.5',", 
        "       '48', '35', '', '', '', '36', '21', '24', '31', '70', '16', '30',", 
        "       '19', '31', '4', '6', '33', '23', '48', '0.67', '28', '18', '34',", 
        "       '33', '', '41', '20', '36', '16', '51', '', '30.5', '', '32', '24',", 
        "       '48', '57', '', '54', '18', '', '5', '', '43', '13', '17', '29', '',", 
        "       '25', '25', '18', '8', '1', '46', '', '16', '', '', '25', '39',", 
        "       '49', '31', '30', '30', '34', '31', '11', '0.42', '27', '31', '39',", 
        "       '18', '39', '33', '26', '39', '35', '6', '30.5', '', '23', '31',", 
        "       '43', '10', '52', '27', '38', '27', '2', '', '', '1', '', '62',", 
        "       '15', '0.83', '', '23', '18', '39', '21', '', '32', '', '20', '16',", 
        "       '30', '34.5', '17', '42', '', '35', '28', '', '4', '74', '9', '16',", 
        "       '44', '18', '45', '51', '24', '', '41', '21', '48', '', '24', '42',", 
        "       '27', '31', '', '4', '26', '47', '33', '47', '28', '15', '20', '19',", 
        "       '', '56', '25', '33', '22', '28', '25', '39', '27', '19', '', '26',", 
        "       '32'], ", 
        "      dtype='|S82')"
       ]
      }
     ], 
     "prompt_number": 51
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "women_only_stats = data[0::,4] == \"female\""
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 34
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "men_only_stats = data[0::,4] != \"female\""
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 35
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "women_onboard = data[women_only_stats,1].astype(np.float)", 
      "men_onboard = data[men_only_stats,1].astype(np.float)"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 56
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "women_onboard.size"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 57, 
       "text": [
        "314"
       ]
      }
     ], 
     "prompt_number": 57
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "men_onboard.size"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 58, 
       "text": [
        "577"
       ]
      }
     ], 
     "prompt_number": 58
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "proportion_women_survived = np.sum(women_onboard) / np.size(women_onboard)"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 59
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "proportion_men_survived = np.sum(men_onboard) / np.size(men_onboard)"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 60
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "proportion_women_survived"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 61, 
       "text": [
        "0.7420382165605095"
       ]
      }
     ], 
     "prompt_number": 61
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "proportion_men_survived"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 62, 
       "text": [
        "0.18890814558058924"
       ]
      }
     ], 
     "prompt_number": 62
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "ls"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "Titanic.ipynb         genderclassmodel.py   gendermodel.py        myfirstforest.py      test.csv", 
        "genderclassmodel.csv  gendermodel.csv       myfirstforest.csv     rules.html            train.csv"
       ]
      }
     ], 
     "prompt_number": 63
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "test_file_object = csv.reader(open('test.csv', 'rb'))"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 105
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "open_file_object = csv.writer(open('genderbasedmodel5.csv', 'wb'))"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 106
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Now we will read in the test file row by row, see if it is femalre or male, and write our survival prediciton to a new file."
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "header = ['PassengerId','Survived']", 
      "header"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 107, 
       "text": [
        "['PassengerId', 'Survived']"
       ]
      }
     ], 
     "prompt_number": 107
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "open_file_object.writerow(header)", 
      "for row in test_file_object:", 
      "    newrow = []", 
      "    if row[3] == 'female':", 
      "        newrow.append(row[0])", 
      "        newrow.append(1)", 
      "        open_file_object.writerow(newrow) #write the row to the new file", 
      "    else:", 
      "        #if he is a dude he probably died, make the prediction", 
      "        newrow.append(row[0])", 
      "        newrow.append(0)", 
      "        open_file_object.writerow(newrow)"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 108
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "pwd"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 103, 
       "text": [
        "u'/Users/johnkabler/kaggle_stuff'"
       ]
      }
     ], 
     "prompt_number": 103
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "ls"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "Titanic.ipynb          genderbasedmodel3.csv  genderclassmodel.csv   gendermodel.py         rules.html", 
        "genderbasedmodel.csv   genderbasedmodel4.csv  genderclassmodel.py    myfirstforest.csv      test.csv", 
        "genderbasedmodel2.csv  genderbasedmodel5.csv  gendermodel.csv        myfirstforest.py       train.csv"
       ]
      }
     ], 
     "prompt_number": 104
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "By now you have created your first python submission.  Let's complicate things and try and submit the same submission as before, binning up the ticket price into the four bins and submitting on class, gender, and ticket price. This part assumes that you have completed the 'Reading in your train.csv' and you have a data array as before.", 
      "", 
      "The idea is to create an table which contains just 1's and 0's. The array will be a survival reference table, whereby you read in the test data, find out passenger attributes, look them up in the survival table, and determine if they survive or not. In the case of a model that uses gender, class, and ticket price, you will need an array of 2x3x4 (male or female, 1st, 2nd, or 3rd class).", 
      "", 
      "The script will systematically will loop through each combination and use the 'where' function in python to search the passengers that fit that combination of variables. Just like before, you can ask what indexes equals female, 1st class, and paid more than $30. The problem is that looping through requires bins of equal sizes, i.e. $0-$9,$10-$19,$20-$29,$30-$39.  For the sake of binning let's say everything equal to and above 40 equals 39 so it falls in this bin, so then you can set the bins:"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "fare_ceiling = 40"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 109
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "data[0::,9].astype(np.float) >= fare_ceiling"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 115, 
       "text": [
        "array([False,  True, False,  True, False, False,  True, False, False,", 
        "       False, False, False, False, False, False, False, False, False,", 
        "       False, False, False, False, False, False, False, False, False,", 
        "        True, False, False, False,  True, False, False,  True,  True,", 
        "       False, False, False, False, False, False, False,  True, False,", 
        "       False, False, False, False, False, False, False,  True, False,", 
        "        True, False, False, False, False,  True, False,  True,  True,", 
        "       False, False, False, False, False, False, False, False,  True,", 
        "        True, False,  True, False, False, False, False, False, False,", 
        "       False, False,  True, False, False, False, False,  True, False,", 
        "       False, False,  True, False, False, False, False,  True, False,", 
        "       False, False, False,  True, False, False, False, False, False,", 
        "       False, False,  True, False, False, False, False, False, False,", 
        "       False,  True, False,  True, False, False, False,  True, False,", 
        "       False, False, False, False, False, False, False, False, False,", 
        "       False, False,  True, False,  True, False, False, False, False,", 
        "       False, False, False, False, False, False, False,  True, False,", 
        "       False, False,  True, False, False, False,  True, False, False,", 
        "       False, False, False, False,  True, False, False,  True, False,", 
        "       False, False, False, False, False, False, False, False, False,", 
        "        True, False, False, False, False,  True, False, False, False,", 
        "       False, False, False, False, False, False,  True, False, False,", 
        "       False, False, False,  True, False, False, False, False, False,", 
        "       False, False, False, False, False, False, False, False,  True,", 
        "       False, False,  True, False, False, False, False, False,  True,", 
        "       False, False, False, False, False,  True, False, False, False,", 
        "       False, False, False, False, False, False, False, False, False,", 
        "       False, False,  True, False, False,  True, False, False, False,", 
        "       False, False, False, False,  True,  True,  True, False, False,", 
        "       False,  True, False, False, False, False, False,  True,  True,", 
        "       False, False, False, False, False,  True, False, False, False,", 
        "       False, False, False, False, False, False, False, False, False,", 
        "       False, False,  True,  True, False, False, False, False, False,", 
        "        True, False,  True, False, False, False, False, False,  True,", 
        "        True,  True, False,  True,  True,  True, False, False, False,", 
        "       False, False, False,  True,  True, False, False, False, False,", 
        "        True,  True, False, False, False,  True, False, False,  True,", 
        "       False,  True, False,  True,  True, False, False, False,  True,", 
        "       False, False, False, False, False, False, False, False, False,", 
        "       False, False, False, False, False,  True, False, False, False,", 
        "       False, False, False, False, False, False,  True, False, False,", 
        "        True,  True, False, False,  True, False,  True, False,  True,", 
        "       False, False,  True, False, False,  True, False,  True,  True,", 
        "       False, False, False,  True, False, False,  True, False, False,", 
        "       False, False, False, False, False, False, False, False, False,", 
        "       False, False, False, False, False, False, False,  True, False,", 
        "       False, False, False, False, False, False, False, False, False,", 
        "       False, False, False, False, False, False, False, False, False,", 
        "       False, False,  True,  True, False, False,  True, False, False,", 
        "       False, False, False, False,  True, False, False, False, False,", 
        "       False, False, False,  True, False, False, False,  True, False,", 
        "       False, False, False, False, False, False, False, False, False,", 
        "       False, False, False, False, False, False, False,  True, False,", 
        "       False, False, False,  True, False, False, False,  True, False,", 
        "        True, False, False, False, False, False, False,  True, False,", 
        "       False,  True, False,  True, False, False, False, False, False,", 
        "        True,  True, False, False, False,  True, False, False, False,", 
        "        True, False, False, False, False, False, False,  True, False,", 
        "       False,  True, False, False, False,  True, False, False, False,", 
        "       False, False, False, False, False, False,  True, False,  True,", 
        "        True, False, False, False,  True, False, False, False, False,", 
        "       False,  True, False, False, False, False, False, False,  True,", 
        "        True, False, False, False, False, False, False, False, False,", 
        "       False, False, False, False,  True, False, False, False, False,", 
        "       False,  True, False, False, False,  True, False,  True, False,", 
        "        True, False,  True, False, False, False,  True, False, False,", 
        "       False, False, False, False, False,  True, False, False,  True,", 
        "       False, False, False, False, False,  True,  True, False, False,", 
        "       False, False, False,  True, False, False, False, False, False,", 
        "        True, False, False, False, False, False,  True, False, False,", 
        "       False, False, False, False, False, False, False, False, False,", 
        "       False, False,  True, False,  True, False,  True, False, False,", 
        "       False, False, False, False, False, False, False,  True, False,", 
        "       False, False,  True,  True, False, False, False, False,  True,", 
        "       False, False, False,  True, False,  True, False, False, False,", 
        "       False, False, False,  True,  True, False,  True, False,  True,", 
        "       False,  True, False, False, False,  True,  True, False,  True,", 
        "       False, False, False, False, False,  True, False,  True, False,", 
        "       False, False, False, False, False, False,  True, False,  True,", 
        "       False,  True, False, False, False,  True, False, False, False,", 
        "       False, False, False, False,  True, False, False, False, False,", 
        "       False,  True, False, False, False, False, False, False,  True,", 
        "       False, False, False,  True,  True, False, False,  True, False,", 
        "       False,  True, False, False, False, False, False,  True, False,", 
        "       False, False, False,  True, False, False, False,  True, False,", 
        "        True, False, False, False, False, False, False, False, False,", 
        "       False, False, False, False, False,  True, False,  True, False,", 
        "       False, False, False, False, False, False,  True, False, False,", 
        "        True, False, False, False, False, False, False, False, False,", 
        "       False,  True, False, False, False, False, False, False,  True,", 
        "       False, False, False, False, False, False, False, False, False,", 
        "       False,  True, False, False, False, False, False,  True, False,", 
        "       False,  True, False, False, False, False, False,  True, False,", 
        "       False,  True, False, False, False, False, False, False, False,", 
        "        True, False, False,  True, False, False, False, False, False,", 
        "       False,  True, False, False, False, False, False, False,  True,", 
        "       False, False, False,  True, False, False, False,  True, False,", 
        "       False, False, False, False, False, False,  True, False, False,", 
        "       False, False, False, False, False, False, False, False, False], dtype=bool)"
       ]
      }
     ], 
     "prompt_number": 115
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Here we are using the above function, which returns True or False depending on the fair price, and finding all fares that exceed the fare ceiling (40 dollars) and setting them to 39 dollars.... The purpose of this is to remove outliers and ensure that we can successfully bucket all fares into the same few categories.  Remember, all models are wrong, but some are useful. "
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "data[data[0::,9].astype(np.float) >= fare_ceiling, 9] = fare_ceiling-1.0"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 117
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "data[0::,9].astype(np.float)"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 118, 
       "text": [
        "array([  7.25  ,  39.    ,   7.925 ,  39.    ,   8.05  ,   8.4583,", 
        "        39.    ,  21.075 ,  11.1333,  30.0708,  16.7   ,  26.55  ,", 
        "         8.05  ,  31.275 ,   7.8542,  16.    ,  29.125 ,  13.    ,", 
        "        18.    ,   7.225 ,  26.    ,  13.    ,   8.0292,  35.5   ,", 
        "        21.075 ,  31.3875,   7.225 ,  39.    ,   7.8792,   7.8958,", 
        "        27.7208,  39.    ,   7.75  ,  10.5   ,  39.    ,  39.    ,", 
        "         7.2292,   8.05  ,  18.    ,  11.2417,   9.475 ,  21.    ,", 
        "         7.8958,  39.    ,   7.8792,   8.05  ,  15.5   ,   7.75  ,", 
        "        21.6792,  17.8   ,  39.6875,   7.8   ,  39.    ,  26.    ,", 
        "        39.    ,  35.5   ,  10.5   ,   7.2292,  27.75  ,  39.    ,", 
        "         7.2292,  39.    ,  39.    ,  27.9   ,  27.7208,  15.2458,", 
        "        10.5   ,   8.1583,   7.925 ,   8.6625,  10.5   ,  39.    ,", 
        "        39.    ,  14.4542,  39.    ,   7.65  ,   7.8958,   8.05  ,", 
        "        29.    ,  12.475 ,   9.    ,   9.5   ,   7.7875,  39.    ,", 
        "        10.5   ,  15.85  ,  34.375 ,   8.05  ,  39.    ,   8.05  ,", 
        "         8.05  ,   7.8542,  39.    ,  20.575 ,   7.25  ,   8.05  ,", 
        "        34.6542,  39.    ,  23.    ,  26.    ,   7.8958,   7.8958,", 
        "        39.    ,   8.6542,   7.925 ,   7.8958,   7.65  ,   7.775 ,", 
        "         7.8958,  24.15  ,  39.    ,  14.4542,   8.05  ,   9.825 ,", 
        "        14.4583,   7.925 ,   7.75  ,  21.    ,  39.    ,  31.275 ,", 
        "        39.    ,   8.05  ,  30.0708,  13.    ,  39.    ,  11.2417,", 
        "         7.75  ,   7.1417,  22.3583,   6.975 ,   7.8958,   7.05  ,", 
        "        14.5   ,  26.    ,  13.    ,  15.0458,  26.2833,  39.    ,", 
        "         9.2167,  39.    ,  15.2458,   7.75  ,  15.85  ,   6.75  ,", 
        "        11.5   ,  36.75  ,   7.7958,  34.375 ,  26.    ,  13.    ,", 
        "        12.525 ,  39.    ,   8.05  ,  14.5   ,   7.3125,  39.    ,", 
        "         7.7333,   8.05  ,   8.6625,  39.    ,  16.1   ,  15.75  ,", 
        "         7.775 ,   8.6625,  39.6875,  20.525 ,  39.    ,  27.9   ,", 
        "        25.925 ,  39.    ,  33.5   ,  29.125 ,  11.1333,   7.925 ,", 
        "        30.6958,   7.8542,  25.4667,  28.7125,  13.    ,   0.    ,", 
        "        39.    ,  15.05  ,  31.3875,  39.    ,  22.025 ,  39.    ,", 
        "        15.5   ,  26.55  ,  15.5   ,   7.8958,  13.    ,  13.    ,", 
        "         7.8542,  26.    ,  27.7208,  39.    ,   7.75  ,   8.4042,", 
        "         7.75  ,  13.    ,   9.5   ,  39.    ,   6.4958,   7.225 ,", 
        "         8.05  ,  10.4625,  15.85  ,  18.7875,   7.75  ,  31.    ,", 
        "         7.05  ,  21.    ,   7.25  ,  13.    ,   7.75  ,  39.    ,", 
        "         7.925 ,  27.    ,  39.    ,  10.5   ,   8.05  ,  13.    ,", 
        "         8.05  ,   7.8958,  39.    ,   9.35  ,  10.5   ,   7.25  ,", 
        "        13.    ,  25.4667,  39.    ,   7.775 ,  13.5   ,  31.3875,", 
        "        10.5   ,   7.55  ,  26.    ,  26.25  ,  10.5   ,  12.275 ,", 
        "        14.4542,  15.5   ,  10.5   ,   7.125 ,   7.225 ,  39.    ,", 
        "         7.775 ,  14.5   ,  39.    ,  26.    ,   7.25  ,  10.4625,", 
        "        26.55  ,  16.1   ,  20.2125,  15.2458,  39.    ,  39.    ,", 
        "        39.    ,  26.    ,   7.75  ,  31.3875,  39.    ,   0.    ,", 
        "         7.75  ,  10.5   ,  39.6875,   7.775 ,  39.    ,  39.    ,", 
        "        31.    ,   0.    ,  19.5   ,  29.7   ,   7.75  ,  39.    ,", 
        "         7.75  ,   0.    ,  29.125 ,  20.25  ,   7.75  ,   7.8542,", 
        "         9.5   ,   8.05  ,  26.    ,   8.6625,   9.5   ,   7.8958,", 
        "        13.    ,   7.75  ,  39.    ,  39.    ,  12.875 ,   8.85  ,", 
        "         7.8958,  27.7208,   7.2292,  39.    ,  30.5   ,  39.    ,", 
        "         7.75  ,  23.25  ,   0.    ,  12.35  ,   8.05  ,  39.    ,", 
        "        39.    ,  39.    ,  24.    ,  39.    ,  39.    ,  39.    ,", 
        "        26.    ,   7.8958,  26.25  ,   7.8542,  26.    ,  14.    ,", 
        "        39.    ,  39.    ,   7.25  ,   7.8958,  12.35  ,  29.    ,", 
        "        39.    ,  39.    ,   6.2375,  13.    ,  20.525 ,  39.    ,", 
        "        23.25  ,  28.5   ,  39.    ,  18.    ,  39.    ,   7.8958,", 
        "        39.    ,  39.    ,   8.05  ,  35.5   ,  26.    ,  39.    ,", 
        "        13.    ,  13.    ,  13.    ,  13.    ,  13.    ,  16.1   ,", 
        "        15.9   ,   8.6625,   9.225 ,  35.    ,   7.2292,  17.8   ,", 
        "         7.225 ,   9.5   ,  39.    ,  13.    ,   7.8792,   7.8792,", 
        "        27.9   ,  27.7208,  14.4542,   7.05  ,  15.5   ,   7.25  ,", 
        "        39.    ,   7.2292,   7.75  ,  39.    ,  39.    ,   6.4958,", 
        "         8.05  ,  39.    ,  21.075 ,  39.    ,   7.25  ,  39.    ,", 
        "         4.0125,   7.775 ,  39.    ,  15.7417,   7.925 ,  39.    ,", 
        "         7.8958,  39.    ,  39.    ,  13.    ,   7.7292,  12.    ,", 
        "        39.    ,   7.7958,   7.925 ,  39.    ,  16.7   ,   7.7958,", 
        "         7.8542,  26.    ,  10.5   ,  12.65  ,   7.925 ,   8.05  ,", 
        "         9.825 ,  15.85  ,   8.6625,  21.    ,   7.75  ,  18.75  ,", 
        "         7.775 ,  25.4667,   7.8958,   6.8583,  39.    ,   0.    ,", 
        "         7.925 ,   8.05  ,  32.5   ,  13.    ,  13.    ,  24.15  ,", 
        "         7.8958,   7.7333,   7.875 ,  14.4   ,  20.2125,   7.25  ,", 
        "        26.    ,  26.    ,   7.75  ,   8.05  ,  26.55  ,  16.1   ,", 
        "        26.    ,   7.125 ,  39.    ,  39.    ,  34.375 ,  18.75  ,", 
        "        39.    ,  10.5   ,  26.25  ,   9.5   ,   7.775 ,  13.    ,", 
        "         8.1125,  39.    ,  19.5   ,  26.55  ,  19.2583,  30.5   ,", 
        "        27.75  ,  19.9667,  27.75  ,  39.    ,   8.05  ,   7.8958,", 
        "        26.55  ,  39.    ,  10.5   ,   7.75  ,  26.55  ,   8.05  ,", 
        "        38.5   ,  13.    ,   8.05  ,   7.05  ,   0.    ,  26.55  ,", 
        "         7.725 ,  19.2583,   7.25  ,   8.6625,  27.75  ,  13.7917,", 
        "         9.8375,  39.    ,  21.    ,   7.0458,   7.5208,  12.2875,", 
        "        39.    ,   0.    ,   8.05  ,   9.5875,  39.    ,  25.4667,", 
        "        39.    ,  29.7   ,   8.05  ,  15.9   ,  19.9667,   7.25  ,", 
        "        30.5   ,  39.    ,   8.05  ,  14.4583,  39.    ,  15.1   ,", 
        "        39.    ,   7.7958,   8.6625,   7.75  ,   7.6292,   9.5875,", 
        "        39.    ,  39.    ,  26.    ,  26.55  ,  22.525 ,  39.    ,", 
        "         7.75  ,   8.05  ,  26.2875,  39.    ,   7.4958,  34.0208,", 
        "        10.5   ,  24.15  ,  26.    ,   7.8958,  39.    ,   7.8958,", 
        "         7.225 ,  39.    ,   7.2292,   7.75  ,  10.5   ,  39.    ,", 
        "         7.925 ,  11.5   ,  26.    ,   7.2292,   7.2292,  22.3583,", 
        "         8.6625,  26.25  ,  26.55  ,  39.    ,  14.5   ,  39.    ,", 
        "        39.    ,  31.275 ,  31.275 ,  26.    ,  39.    ,  26.    ,", 
        "        26.    ,  13.8625,  20.525 ,  36.75  ,  39.    ,  26.    ,", 
        "         7.8292,   7.225 ,   7.775 ,  26.55  ,  39.6   ,  39.    ,", 
        "        39.    ,  17.4   ,   7.75  ,   7.8958,  13.5   ,   8.05  ,", 
        "         8.05  ,  24.15  ,   7.8958,  21.075 ,   7.2292,   7.8542,", 
        "        10.5   ,  39.    ,  26.3875,   7.75  ,   8.05  ,  14.5   ,", 
        "        13.    ,  39.    ,  14.4583,   7.925 ,  30.    ,  39.    ,", 
        "        26.    ,  39.    ,   8.7125,  39.    ,  15.    ,  39.    ,", 
        "         8.05  ,   8.05  ,   7.125 ,  39.    ,   7.25  ,   7.75  ,", 
        "        26.    ,  24.15  ,  33.    ,   0.    ,   7.225 ,  39.    ,", 
        "        27.    ,   7.8958,  39.    ,   8.05  ,  26.55  ,  15.55  ,", 
        "         7.8958,  30.5   ,  39.    ,  39.    ,  31.275 ,   7.05  ,", 
        "        15.5   ,   7.75  ,   8.05  ,  39.    ,  14.4   ,  16.1   ,", 
        "        39.    ,  10.5   ,  14.4542,  39.    ,  15.7417,   7.8542,", 
        "        16.1   ,  32.3208,  12.35  ,  39.    ,   7.8958,   7.7333,", 
        "        30.    ,   7.0542,  30.5   ,   0.    ,  27.9   ,  13.    ,", 
        "         7.925 ,  26.25  ,  39.6875,  16.1   ,   7.8542,  39.    ,", 
        "        27.9   ,  39.    ,  19.2583,  39.    ,   7.8958,  35.5   ,", 
        "         7.55  ,   7.55  ,   7.8958,  23.    ,   8.4333,   7.8292,", 
        "         6.75  ,  39.    ,   7.8958,  15.5   ,  13.    ,  39.    ,", 
        "        39.    ,   7.225 ,  25.5875,   7.4958,   7.925 ,  39.    ,", 
        "        13.    ,   7.775 ,   8.05  ,  39.    ,  39.    ,  39.    ,", 
        "        10.5   ,  13.    ,   0.    ,   7.775 ,   8.05  ,   9.8417,", 
        "        39.    ,  39.    ,   8.1375,  39.    ,   9.225 ,  39.    ,", 
        "        39.    ,  39.    ,  39.6875,  10.1708,   7.7958,  39.    ,", 
        "        39.    ,  13.4167,  39.    ,   7.225 ,  26.55  ,  13.5   ,", 
        "         8.05  ,   7.7333,  39.    ,   7.65  ,  39.    ,  26.2875,", 
        "        14.4542,   7.7417,   7.8542,  26.    ,  13.5   ,  26.2875,", 
        "        39.    ,  15.2458,  39.    ,  26.55  ,  39.    ,   9.4833,", 
        "        13.    ,   7.65  ,  39.    ,  10.5   ,  15.5   ,   7.775 ,", 
        "        33.    ,   7.0542,  13.    ,  13.    ,  39.    ,   8.6625,", 
        "        21.    ,   7.7375,  26.    ,   7.925 ,  39.    ,  18.7875,", 
        "         0.    ,  13.    ,  13.    ,  16.1   ,  34.375 ,  39.    ,", 
        "         7.8958,   7.8958,  30.    ,  39.    ,  39.    ,  16.1   ,", 
        "         7.925 ,  39.    ,  20.25  ,  13.    ,  39.    ,   7.75  ,", 
        "        23.    ,  12.475 ,   9.5   ,   7.8958,  39.    ,  14.5   ,", 
        "         7.7958,  11.5   ,   8.05  ,  39.    ,  14.5   ,   7.125 ,", 
        "         7.2292,  39.    ,   7.775 ,  39.    ,  39.6   ,   7.75  ,", 
        "        24.15  ,   8.3625,   9.5   ,   7.8542,  10.5   ,   7.225 ,", 
        "        23.    ,   7.75  ,   7.75  ,  12.475 ,   7.7375,  39.    ,", 
        "         7.2292,  39.    ,  30.    ,  23.45  ,   7.05  ,   7.25  ,", 
        "         7.4958,  29.125 ,  20.575 ,  39.    ,   7.75  ,  26.    ,", 
        "        39.    ,  30.6958,   7.8958,  13.    ,  25.9292,   8.6833,", 
        "         7.2292,  24.15  ,  13.    ,  26.25  ,  39.    ,   8.5167,", 
        "         6.975 ,   7.775 ,   0.    ,   7.775 ,  13.    ,  39.    ,", 
        "         7.8875,  24.15  ,  10.5   ,  31.275 ,   8.05  ,   0.    ,", 
        "         7.925 ,  37.0042,   6.45  ,  27.9   ,  39.    ,   8.6625,", 
        "         0.    ,  12.475 ,  39.6875,   6.95  ,  39.    ,  37.0042,", 
        "         7.75  ,  39.    ,  14.4542,  18.75  ,   7.2292,   7.8542,", 
        "         8.3   ,  39.    ,   8.6625,   8.05  ,  39.    ,  29.7   ,", 
        "         7.925 ,  10.5   ,  31.    ,   6.4375,   8.6625,   7.55  ,", 
        "        39.    ,   7.8958,  33.    ,  39.    ,  31.275 ,   7.775 ,", 
        "        15.2458,  39.4   ,  26.    ,   9.35  ,  39.    ,  26.55  ,", 
        "        19.2583,   7.2292,  14.1083,  11.5   ,  25.9292,  39.    ,", 
        "        13.    ,  13.    ,  13.8583,  39.    ,   9.5   ,  11.1333,", 
        "         7.8958,  39.    ,   5.    ,   9.    ,  24.    ,   7.225 ,", 
        "         9.8458,   7.8958,   7.8958,  39.    ,  26.    ,   7.8958,", 
        "        10.5167,  10.5   ,   7.05  ,  29.125 ,  13.    ,  30.    ,", 
        "        23.45  ,  30.    ,   7.75  ])"
       ]
      }
     ], 
     "prompt_number": 118
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "fare_bracket_size = 10", 
      "number_of_price_brackets = fare_ceiling / fare_bracket_size", 
      "# this, with a fare_ceiling of 40, creates 4 ticket price buckets", 
      "number_of_classes = 3 # there were 1st, 2nd, and 3rd classes on Titanic: data[2]", 
      "#define the survival table", 
      "survival_table = np.zeros((2, number_of_classes, number_of_price_brackets))", 
      ""
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 121
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "survival_table"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 122, 
       "text": [
        "array([[[ 0.,  0.,  0.,  0.],", 
        "        [ 0.,  0.,  0.,  0.],", 
        "        [ 0.,  0.,  0.,  0.]],", 
        "", 
        "       [[ 0.,  0.,  0.,  0.],", 
        "        [ 0.,  0.,  0.,  0.],", 
        "        [ 0.,  0.,  0.,  0.]]])"
       ]
      }
     ], 
     "prompt_number": 122
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Now lets loop through each variable and find all those passengers that agree with the statements:"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "for i in xrange(number_of_classes): #search through each class", 
      "    for j in xrange(number_of_price_brackets): #search through each price for each class", 
      "        ", 
      "        women_only_stats = data[                  #which element", 
      "                                (data[0::,4] == \"female\") #is a female", 
      "                                &(data[0::,2].astype(np.float) == i+1) # in ith class", 
      "                                &(data[0::,9].astype(np.float) >= j*fare_bracket_size) ", 
      "                                &(data[0::,9].astype(np.float) < (j+1)*fare_bracket_size)", 
      "                                ,1] #in 1st column", 
      "        ", 
      "        men_only_stats = data[", 
      "                              (data[0::,4] != \"female\") ", 
      "                              &(data[0::,2].astype(np.float) == i+1)", 
      "                              &(data[0::,9].astype(np.float) >= j*fare_bracket_size) ", 
      "                              &(data[0::,9].astype(np.float) < (j+1)*fare_bracket_size)", 
      "                              ,1] #in 1st column", 
      "        ", 
      "        survival_table[0,i,j] = np.mean(women_only_stats.astype(np.float))", 
      "        ", 
      "        survival_table[1,i,j] = np.mean(men_only_stats.astype(np.float))", 
      "                              "
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 126
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "survival_table"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 127, 
       "text": [
        "array([[[        nan,         nan,  0.83333333,  0.97727273],", 
        "        [        nan,  0.91428571,  0.9       ,  1.        ],", 
        "        [ 0.59375   ,  0.58139535,  0.33333333,  0.125     ]],", 
        "", 
        "       [[ 0.        ,         nan,  0.4       ,  0.38372093],", 
        "        [ 0.        ,  0.15873016,  0.16      ,  0.21428571],", 
        "        [ 0.11153846,  0.23684211,  0.125     ,  0.24      ]]])"
       ]
      }
     ], 
     "prompt_number": 127
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "survival_table[ survival_table != survival_table ] = 0"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 128
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "survival_table"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 129, 
       "text": [
        "array([[[ 0.        ,  0.        ,  0.83333333,  0.97727273],", 
        "        [ 0.        ,  0.91428571,  0.9       ,  1.        ],", 
        "        [ 0.59375   ,  0.58139535,  0.33333333,  0.125     ]],", 
        "", 
        "       [[ 0.        ,  0.        ,  0.4       ,  0.38372093],", 
        "        [ 0.        ,  0.15873016,  0.16      ,  0.21428571],", 
        "        [ 0.11153846,  0.23684211,  0.125     ,  0.24      ]]])"
       ]
      }
     ], 
     "prompt_number": 129
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "survival_table[ survival_table < .5 ] = 0", 
      "survival_table"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 130, 
       "text": [
        "array([[[ 0.        ,  0.        ,  0.83333333,  0.97727273],", 
        "        [ 0.        ,  0.91428571,  0.9       ,  1.        ],", 
        "        [ 0.59375   ,  0.58139535,  0.        ,  0.        ]],", 
        "", 
        "       [[ 0.        ,  0.        ,  0.        ,  0.        ],", 
        "        [ 0.        ,  0.        ,  0.        ,  0.        ],", 
        "        [ 0.        ,  0.        ,  0.        ,  0.        ]]])"
       ]
      }
     ], 
     "prompt_number": 130
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "survival_table[ survival_table >= .5 ] = 1"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 131
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "survival_table"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 139, 
       "text": [
        "array([[[ 0.,  0.,  1.,  1.],", 
        "        [ 0.,  1.,  1.,  1.],", 
        "        [ 1.,  1.,  0.,  0.]],", 
        "", 
        "       [[ 0.,  0.,  0.,  0.],", 
        "        [ 0.,  0.,  0.,  0.],", 
        "        [ 0.,  0.,  0.,  0.]]])"
       ]
      }
     ], 
     "prompt_number": 139
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "ls"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "Titanic.ipynb          genderbasedmodel3.csv  genderclassmodel.csv   gendermodel.py         rules.html", 
        "genderbasedmodel.csv   genderbasedmodel4.csv  genderclassmodel.py    myfirstforest.csv      test.csv", 
        "genderbasedmodel2.csv  genderbasedmodel5.csv  gendermodel.csv        myfirstforest.py       train.csv"
       ]
      }
     ], 
     "prompt_number": 140
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "test_file_object = csv.reader(open('test.csv', 'rb'))", 
      "fname = 'genderclassbasedpricemodel.csv'", 
      "open_file_object = csv.writer(open(fname,'wb'))", 
      "header2 = test_file_object.next()", 
      "header2"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 144, 
       "text": [
        "['PassengerId',", 
        " 'Pclass',", 
        " 'Name',", 
        " 'Sex',", 
        " 'Age',", 
        " 'SibSp',", 
        " 'Parch',", 
        " 'Ticket',", 
        " 'Fare',", 
        " 'Cabin',", 
        " 'Embarked']"
       ]
      }
     ], 
     "prompt_number": 144
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "column 0: PassengerId", 
      "column 1: Pclass", 
      "column 2: Name", 
      "column 3: Sex", 
      "column 9: Fare"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "header3 = ['PassengerId','Survived']", 
      "open_file_object.writerow(header3)", 
      "for row in test_file_object:", 
      "    #We are going to loop through each passenger in the test set", 
      "    for j in xrange(number_of_price_brackets): ", 
      "        #For each passenger we loop through each price bin", 
      "        #Some passengers have no price data so TRY to make a float", 
      "        try:", 
      "            row[9] = float(row[9])", 
      "            #if it fails, no data so bin fare according to class", 
      "        except:", 
      "            bin_fare = 3-float(row[1])", 
      "            break", 
      "        if row[9] > fare_ceiling: ", 
      "            #if there is data see if it is greater than fare_ceiling previously set", 
      "            #if so set to highest bin and break bin loop", 
      "            bin_fare = number_of_price_brackets - 1", 
      "            break", 
      "        if row[9] >= j*fare_bracket_size and row[9] < (j+1)*fare_bracket_size:", 
      "            # if fare that is 7 dollars is greater than 0 * 10 and", 
      "            # less than 1 * 10, it will be assigned to the index of bin_fare", 
      "            # equal to zero.  ", 
      "            # a 12 dollar fare would be given a bin_fare index of 1, and so on", 
      "            ", 
      "            bin_fare = j  # then assign index", 
      "            break", 
      "            ", 
      "    if row[3] == 'female':  #if passenger is female", 
      "        row2 = []", 
      "        row2.insert(0, row[0])", 
      "        row2.insert(1, int(survival_table[0, float(row[1])-1, bin_fare]))", 
      "        open_file_object.writerow(row2)", 
      "    else:", 
      "        row2 = []", 
      "        row2.insert(0, row[0])", 
      "        row2.insert(1, int(survival_table[1, float(row[1])-1, bin_fare]))", 
      "        open_file_object.writerow(row2)", 
      "           ", 
      "            ", 
      "            "
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 145
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "pwd"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 146, 
       "text": [
        "u'/Users/johnkabler/kaggle_stuff'"
       ]
      }
     ], 
     "prompt_number": 146
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [], 
     "language": "python", 
     "outputs": []
    }
   ]
  }
 ]
}